# Variational Auto-encoder

##  Question: 1) how to define the latent variables z (i.e., decide what information they represent), and 2) how to deal with the integral over z. 

1)They assume that there is no simple interpretation
of the dimensions of z, and instead assert that samples of z can be drawn
from a simple distribution , i.e., N ( 0, I )
* The key is to notice that any distribution in d dimensions can
be generated by taking a set of d variables that are normally distributed and
mapping them through a sufficiently complicated function
* For example, say we wanted to construct a 2D random variable whose values lie on a
ring. If z is 2D and normally distributed, g ( z ) = z/10 + z/ || z || is roughly
ring-shaped
* In fact, recall that P ( X | z; θ ) = N ( X | f ( z; θ ) , σ 2 ∗ I ) . If f ( z; θ ) is a multi-layer neural network(Encoder),
then we can imagine the network using its first few layers to map the normally distributed z’s to the latent values with exactly the right statitics. Then it can use later layers (Decoder) to map
those latent values to a fully-rendered digit.

2) It is actually conceptually straightforward to compute P ( X ) approximately: we first sample a large
number of z values { z 1 , ..., z n } , and compute P ( X ) ≈ n 1 ∑ i P ( X | z i ) . The problem here is that in high dimensional spaces, n might need to be extremely
large before we have an accurate estimate of P ( X ) .
* Even if our model is an accurate generator of digits, we would likely need to
sample many thousands of digits before we produce a X' that is sufficiently
similar to the X. VAEs alter the sampling procedure to make it faster, without changing the similarity
metric.

## Setting up Objective (The relationship between E z ∼ q P ( X | z ) and P ( X ))
1) D [ Q ( z ) || P ( z | X )] = E z ∼ Q [ log Q ( z ) − log P ( z | X )]
2) By applying Bayes Rule: log P ( X ) − D [ Q ( z ) || P ( z | X )] = E z ∼ Q [ log P ( X | z )] − D [ Q ( z )|| P ( z )]
3) Adding dependency on X for Q:  **log P ( X ) − D [ Q ( z| X ) || P ( z | X )] = E z ∼ Q [ log P ( X | z )] − D [ Q ( z| X )|| P ( z )]**
* Left: We want to maximize P(X).  Also match Q(z|X) (tractable) to P(z|X) (intractable), which means minimizing KL (Q(z|X)  ||   P(z|X)) to zero.
* RIght: Sth we can optimize via SGD give a right Q

## Optimize Objective
1) First we need to be a bit more specific about the form that Q ( z | X ) will take. The usual choice is to say that Q ( z | X ) = N ( z | μ ( X; θ ) , Σ ( X; θ ))
2) The last term— D [ Q ( z | X )|| P ( z )] —is now a KL-divergence between two multivariate Gaussian distributions, which can be computed in closed form
3) THr first term E z ∼ Q [ log P ( X | z )]. we take one sample of z and treat P ( X | z ) for that z as an approximation of
E z ∼ Q [ log P ( X | z )] 
* Because we are already doing stochastic gradient descent over different values of X sampled from a dataset D
4) Therefore, we can sample a single value of X and a single value of z from the distribution Q ( z | X ) , and compute the gradient of:  log P ( X | z ) − D [ Q ( z | X ) || P ( z )] THen average thie gradient of this function over arbitrarily many
samples of X and z, and the result converges to the gradient of Equation: E X ∼ D [ E z ∼ Q [ log P ( X | z )] − D [ Q ( z | X ) || P ( z )]]
5) reparameterization trick: Because we sample z from Q(z|X) which has parameters that we need to learn in Neural Nework, and then need back-prop. However, Q is non-continuous operatioConditional Variational Autoencodersn and has no gradient. reparameterization trick replace the distribution that depend on our model parameters with a given function. 

6) Right hand side is a lower bound to P(X). 

## Interpreting Objective
1) we ask how much error is introduced by optimizing D[ Q ( z | X ) ||P ( z | X )] in addition to log P ( X ) . 
2) we describe the VAE framework—especially the r.h.s. of Equation 5—in terms of information theory, linking it to other approaches based on Minimum Description Length.
* We first use some bits to construct z. Recall that a KL-divergence is measured in bits (or nats). We measure the bits required to construct z using a D[ Q ( z | X )|| P ( z )] because under our model, we assume
that any z which is sampled from P ( z ) = N ( z | 0, I ) can contain no informa-
tion about X. Hence, we need to measure the amount of extra information
that we get about X when z comes from Q ( z | X ) instead of from P ( z )
* In the second step, P ( X | z ) measures the amount of information required to reconstruct X from
z under an ideal encoding. Hence, the total number of bits (log P ( X ) ) is the
sum of these two steps, minus a penalty we pay for Q being a sub-optimal
encoding ( D[ P ( z | X )|| Q ( z | X )] ).

3) Finally, we investigate whether VAEs have “regularization parameters” analogous to the sparsity penalty in sparse autoencoders.
------
# Conditional Variational Autoencoders
